#+PROPERTY: header-args :eval never-export
#+HUGO_BASE_DIR: ../
#+HUGO_SECTION: posts

#+TITLE: Studying Support Vector Machines

#+AUTHOR: Nikola Stoyanov
#+EMAIL: nikst@posteo.net
#+DATE: <2018-11-20 Tue>

#+HUGO_TAGS: "support vector machine" "data science"  "numerical optimization"
#+HUGO_CATEGORIES: "machine learning" "python"
#+HUGO_DRAFT: false

#+HUGO_MENU: :menu "posts" :weight 2001

#+STARTUP: showall
#+STARTUP: showstars
#+STARTUP: inlineimages

* Introduction
Having used support vector machines (SVM) for classification problems
I decided to learn the theory behind them the lack of which is
probably the principal reason for my poor results. Following my study
I took notes in jupyter which I eventually converted into orgmode and
now decided to turn into a blog post. The structure of this post is:

- Theory : mathematical formulation of SVM and solver
- Visualisation : python code for exploring SVM
- Exploration : show how SVM work in practise

* Theory
We start off with the idea that we want to separate a space by
decision boundaries in order to carry out classification. Hyperplanes
of an n-dimensional space $V$ are a subspace with dimension of
$n-1$. You can also see people referring to this as having a
codimension 1 in $V$ which simply means the difference between the
dimensions of the space and the smaller object contained in it. To
define a plane we need points on the plane in SVM we do this using
$\mathbf{x}$ which is a vector to such a point and a unit normal to
the plane ($\mathbf{w}$). Now for a space with $n=2$ dimensions we can
describe a the decision boundary as a hyperplane using Equation
ref:eq:origin_vector.

\begin{equation}
\label{eq:origin_vector}
\mathbf{w} \cdot \mathbf{x} = 0
\end{equation}

Graphically this is shown in Figure ref:origin_vector.

#+LATEX_ATTR: :placement [H]
#+CAPTION: Orthogonality between vectors
#+NAME: origin_vector
#+ATTR_LATEX: :width 10cm
#+ATTR_ORG: :width 500
#+ATTR_HTML: :width 500
[[./img/origin_vector.svg]]

The vectors $\mathbf{w}$ and $\mathbf{x}$ are orthogonal (the dot
product is zero). Using the above equation we can get other vectors
pointing to points which when dotted with $\mathbf{w}$ will give zero
and define the separating hyperplane. However, this is not a
generalized case. To see why lets consider what we are trying to do:
we need a plane that can pass anywhere in the space and in the above
case we can only go through the origin which will limit us in the
ability to form a decision boundary.

To generalize we need to allow $\mathbf{x}$ to pass through any point
on the coordinate system. We can do this by translating the vector
$\mathbf{x}$ by another vector $\mathbf{a}$. Note that $\mathbf{w}$
and $\mathbf{x}$ are still orthogonal to each other even after the
translation since the dot product considers the projection of the
vectors. Equation ref:eq:origin_vector is now shifted and shown in
Equation ref:eq:arbitrary_vector.

\begin{equation}
\label{eq:arbitrary_vector}
\mathbf{w}\cdot (\mathbf{x-a})=0
\end{equation}

Equation ref:eq:arbitrary_vector can is graphically shown in Figure
ref:arbitrary_vector.

#+LATEX_ATTR: :placement [H]
#+CAPTION: Arbitrary decision boundary
#+NAME: arbitrary_vector
#+ATTR_LATEX: :width 10cm
#+ATTR_ORG: :width 500
#+ATTR_HMTL: :width 500
[[./img/arbitrary_vector.svg]]

We can now see that the decision boundary does not necessarily need to
pass through the origin. To simplify the equation lets multiply the
terms.

$$\mathbf{w}\cdot \mathbf{x} - \mathbf{w}\cdot \mathbf{a} = 0$$

The dot product of $\mathbf{w}$ and $\mathbf{a}$ gives a scalar which we can dubiously call $-b$. Hence we get

$$\mathbf{w}\cdot \mathbf{x} + b = 0$$

For multi dimensional case the constant $\mathbf{w}_n$ and vectors to
the data points $\mathbf{x}_n$ are each of the dimension of the space
$V$. The equation can now be written as

$$\sum_n \mathbf{w}_n \mathbf{x}_n + b= 0$$

Or

$$w_1 x_1 + w_2 x_2 + \dots + w_n x_n + b= 0$$

Decision rules can now be written as a vector to a point passed
through the equation will produce a value that will be higher or lower
than the costant $b$. We can then classify the data point based on
this rule.

Great so now we can define a separating plane which will divide the
space and allows us to classifiy data. So lets look into what SVM are
and how they make use of this. SVM is a technique for binary data
classification. The formulation can be extended to multi-class
classification and regression as I will show in the end. If we have a
training set with labels and features then SVM can create a model
which depends only on the most critical points called support vectors
and predict the labels of new data. It is useful to define what is it
that we want to achieve now since most of the work in SVM is to
transform the initial solution into a useful solution. That is we are
looking for an analytical linear convex optimization problem which has
the ability to easily map the features to different spaces. The
benefit being:
- Analytical: we do not have to use numerical methods to solve the
  equations. Numerical methods come at a cost of computation time,
  convergence issues and additional hyperparameters.
- Linear: linear problems are easier to express mathematically and
  thus to compute. This is extremely important when we want to change
  the feature space.
- Convex: convex optimization problems quarantee than a local minima
  is a global minima and that first order conditions are sufficient
  for optimality. In other words setting the first order derivative to
  zero is enough for the optimizer to find the global mimima, unlike
  neural networks where we have no such benefits.

Given a training set of feature and label pairs $(\mathbf{x}_i, y_i),
i=1,\ldots,N$ where $N$ is the number of data points, the features
$\mathbf{x}_i \in R^n$ ($\mathbf{x}_i$ is a vector holding the
features of a data point) and labels $y \in (-1, 1)$ the support
vector machine finds a hyperplane which separates the classes by
minimizing the error and maximizing the perpendicular distance between
the closest (most critical) points
https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf. The
decision function is then fully specified by a subset of training
samples known as support vectors. Using the terminology of Professor
Winston shown here 1 https://www.youtube.com/watch?v=_PwhiWxHK8o,
the support vectors are the gutters of the street with the center line
being the hyperplane (in 2D this is just a line). The equations for
the support vectors are given below, taken from the amazingly written
article titled "An Idiotâ€™s guide to Support vector machines (SVMs)"
http://web.mit.edu/6.034/wwwbob/svm.pdf - this was perfect for me
:)

\begin{aligned}
\mathbf{w}.\mathbf{x}_i + b = 1 && \text{support vector for } y_i = 1\\
\mathbf{w}.\mathbf{x}_i + b = 0 && \text{street center line}\\
\mathbf{w}.\mathbf{x}_i + b = -1 && \text{support vector for } y_i = -1
\end{aligned}

What the above means is that if we get a vector $\mathbf{x}_i$ which
points to a point on the hyperplane and the inner product (dot
product) with the normal to the hyperplane it will give a constant
$b$. Mathematically $\mathbf{w} \cdot \mathbf{x}_i
= ||\mathbf{w}||. ||\mathbf{x}_i|| cos(\theta) = -b$, where $ ||.|| $
denotes the Euclidian distance and $cos(\theta)$ is the cosine of the
smaller angle between the two vectors.

The other two equations are for the decision boundaries (hyperplanes
that go through the closest points) and we just define that they give
$1-b$ and $-1-b$. Why? If we chose $5-b$ and $-5-b$ and are consistent
the constant factor will be dropped in the optization problem as shown
below.

Also assuming that no data points exist between the gutters i.e. our
error has to be $0$, we get the following:

\begin{aligned}
\mathbf{w}.\mathbf{x}_i + b \geq 1 && ,\text{if} && y_i = 1\\
\mathbf{w}.\mathbf{x}_i + b \leq -1 && ,\text{if} && y_i = -1
\end{aligned}

Or we can combine them into one equation by using the variable $y_i$
as such: $y_i(\mathbf{x}_i.\mathbf{w})\geq 1$.

The above definition gives an inifite number of hyperplanes since
there are infinite values for $b$, therefore, we need more
constraints. In SVM the additional constraint is that we choose the
hyperplane which maximizes the distance between the decision
boundaries. We wish to maximize the distance between the gutters
(support vectors) and the center line of the street (hyperplane). This
is the Euclidian distance expressed below for half of the street. We
take the modulus of the of hyperplane since we are interested in the
distance.

$$\frac{|\mathbf{w}.\mathbf{x}+b|}{||\mathbf{w}||} = \frac{|+1|}{||\mathbf{w}||} = \frac{|-1|}{||\mathbf{w}||} = \frac{1}{||\mathbf{w}||}$$

Due to symmetry the total distance is then given by

$$\frac{2}{||\mathbf{w}||}$$

To visualize what we have until now I have largely adapted a tikz
script from Yifan Peng which you can find [[http://blog.pengyifan.com/tikz-example-svm-trained-with-samples-from-two-classes/][here]].

#+LATEX_ATTR: :placement [H]
#+CAPTION: Support vector decision boundary
#+NAME: svg
#+ATTR_LATEX: :width 10cm
#+ATTR_ORG: :width 500
[[./img/svm.svg]]

We can now follow the logical sequence than since it ok to maximize
$2/||w||$ it is ok to maximize $1/||w||$ and minimize
$||w||$. Finally, for mathematical convenience it is ok to minimize
$\frac{1}{2}||w||^2$. This mathematical convenience lies in the fact
that the least squares error coincides with the maximum likelihood
estimates if we assume that the errors are independent, normally
distributed, with zero mean and have equal variances. Our optimization
problem then becomes:

$$\arg \underset{\mathbf{w}} {min} \frac{1}{2}||\mathbf{w}||^2$$

This is a quadratic constraint optimization problem which means that
the surface if quadratic. This is great news since quadratic surfaces
have a single extremum (minama/maxima). Or otherwise said, our solver
is guaranteed to not get trapped in local minimas unlike navigating
through neural networks. A two dimensional case with features called
$x_1$ and $x_2$ is shown below. The image is blatantly taken from [[https://en.wikipedia.org/wiki/Lagrange_multiplier][here]]
wikipedia article.

#+LATEX_ATTR: :placement [H]
#+CAPTION: Lagrange multiplier
#+NAME: lagrange_multiplier
#+ATTR_LATEX: :width 10cm
#+ATTR_ORG: :width 500
[[./img/LagrangeMultipliers2D.svg]]

This is a constraint optimization problem. As with all optimization
problems it can be solved numerically, however, numerical solutions
introduce a whole host of other problems such as issues with
convergence and additional hyperparameters. It turns out that there is
a better approach and an analytical solution can be found. We are
looking for the maxima of $f$ to exist on $g$. If we superimpose the
two functions, the gradients must line up, otherwise there is no
solution. The gradient will return the direction in which a function
increases most rappidly (steepest ascent in this case). Now we require
that the two gradients are in the same direction and therefore they
must be multiples of each other. The constant which gives the equality
is called a Lagrange multiplier and is needed because the directions
of the vectors are parallel but the magnitudes as
different. Mathematically this can be expressed as below.

$$\nabla f = -\lambda \nabla g$$

Where $\lambda$ is the Lagrange multiplier and $\nabla$ is the
gradient given by $\nabla = \frac{\partial}{\partial x^i}
\mathbf{e}^i$. The negative sign of $\lambda$ is arbitrary and the
derivation can be done with a positive sign. It really does not matter
since $\lambda$ is a constant.

A little more explanation on the above is that at the solution point
(assuming we have somehow found it) the constraint is tangent to the
surface. This is what having a solution means. Now the derivatives of
$f$ and $g$ will both point towards the extremum of $f$ which is the
maxima in this case. There is no other way if a solution exists since
we superimposed $g$ on $f$.

Rearranging gives

$$\frac{\partial f}{\partial x^i} + \lambda \frac{\partial g}{\partial x^i} = 0$$

We also need a second constraint which will we require that the
solution also lie on the constraint which is obvious but necessary
condition. This is because the above equation only guarantees that the
gradients are in the same direction which is neccessary but not
sufficient and we will get infinite solutions. To get a unique answer
we need both as such $g(\mathbf{x^i}) = 0$. For the derivatives to
work we also need to assume the functions have continuous first
derivatives $f, g \in C^1$. The problem can now be expressed as
optimization of the Lagrangian $\mathcal{L}$.

\begin{equation}
\mathcal{L} (x, \lambda) = f(x) + \lambda g(x)\\
\nabla \mathcal{L} = 0
\end{equation}

The above is exaclty the same formulation as before as quickly shown
below.

\begin{equation}
\begin{aligned}
\nabla \mathcal{L} & = 0\\
\frac{\partial \mathcal{L}}{\partial x} & = 0 = \frac{\partial f}{\partial x} + \frac{\partial \lambda g}{\partial x} = \frac{\partial f}{\partial x} + \lambda \frac{\partial g}{\partial x}\\
\frac{\partial \mathcal{L}}{\partial \lambda} & = 0 = \frac{\partial f}{\partial \lambda} + \frac{\partial \lambda g}{\partial \lambda} = 0 + g(x) = g(x)
\end{aligned}
\end{equation}

We can now generalize the expression to multiple constraints.

$$\mathcal{L} (x^i, \lambda) = f(x^i) + \sum_i \lambda_i g_i(x^i)$$

We can now implement the Lagrangian for SVM. The optimization problem is shown below.

\begin{eqquation}
\begin{aligned}
\arg  \underset{{\mathbf{w}}}{min} && \frac{1}{2} ||\mathbf{w}||^2\\
\text{subject to} && y_i (\mathbf{w} \cdot \mathbf{x}_i + b) = 1
\end{aligned}
\end{equation}

The Lagrangian is therefore given by the following over $N$ training
points. This is known as the primal optimization problem. Later we are
going to derive the dual optimization which is what we will use.

\begin{equation}
\begin{aligned}
\arg \underset{{\mathbf{w},b}} {min}  \mathcal{L}_p (x^i, b, \lambda) = & \frac{1}{2} ||\mathbf{w}||^2 - \sum_i \lambda_i y_i (\mathbf{w} \cdot \mathbf{x}_i + b) \\
 =& \frac{1}{2} ||\mathbf{w}||^2 - \sum_{i=1}^N {\lambda_i y_i (\mathbf{w} \cdot \mathbf{x}_i + b)} + \sum_{i=1}^N {\lambda_i}
\end{aligned}
\end{equation}

To minimize get the derivates with respect to $x$ and $b$ and set to zero.

\begin{equation}
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial w} & = \mathbf{w} - \sum_{i=1}^N \lambda_i y_i \mathbf{x}_i = 0\\
\frac{\partial \mathcal{L}}{\partial b} & = \sum_{i=1}^{N} \lambda_i y_i = 0\\
\end{aligned}
\end{equation}

Therefore

\begin{equation}
\begin{aligned}
& \mathbf{w} = \sum_{i=1}^N \lambda_i y_i \mathbf{x}_i \\
& \sum_{i=1}^N \lambda_i y_i = 0
\end{aligned}
\end{equation}

The above gives us an extremely important insight into the problem. We
now know that the weights $w$ are a linear combination of the
features, $x$, the labels, $y$ and the Lagrangian multipliers,
$\lambda_i$.

We can now solve the above problem and get our results. However, we
will not have achieved one of our criteria at the start and that is
the ability to innately handle different topological spaces which will
be extremely limiting as will be shown later. The above equations
depend on $w$ and $b$ which is normal since this is what we derived :
). To allow the space to be transformed it is very desirable to remove
that dependence. Luckily there is the Kuhn-Tucker theorem which
states:

So substituting back into the primal optimization problem we obtain

\begin{equation}
\begin{aligned}
\mathcal{L}_p (x^i, b, \lambda) = & \frac{1}{2} ||\mathbf{w}||^2 - \sum_{i=1}^N {\lambda_i y_i (\mathbf{w} \cdot \mathbf{x}_i + b)} + \sum_{i=1}^N {\lambda_i} \\
= &  \frac{1}{2} 
\end{aligned}
\end{equation}

$$\sum_{i=1}^N \lambda_i y_i = 0$$

Solving the dual optimization problem we obtain the Lagrangian
multipliers! Literature tells us only the support vectors (in the
street gutter) will be non-zero. This is important so we can check it
later in scikit-learn. From here we can calculate the weights as below

$$\mathbf{w} = \sum_{i=1}^N \lambda_i y_i \mathbf{x}_i$$

Following from the Lagrangian multipliers most of the weights will
also be zero. Only the ones associated with the support vectors will
have non-zero values.

Therefore, having trained on some linearly separable data we can
classify an unknown points $p$ as below

$$f(x) = \mathbf{w} \cdot \mathbf{p} + b = \sum_{i=1}^N (\lambda_i y_i \mathbf{x}_i \cdot \mathbf{p}) + b$$

The classification is then determined by the sign of $f(x)$.

$$sgn f(x)$$

In scikit-learn there is one more term, $C$, added to the optimization
which takes care of data that is impossible to separate with a given
kernel. This is likely to occur since data from the real world is
likely to be noisy. This parameter controls how much error we tolerate
and in essence determines the trade-off between the bias and variance
of our classifier. Unfortunately, $C$ is a hyperparameter and needs to
be tuned. The modified equations are shown below.

\begin{equation}
\begin{aligned}
\arg  \underset{{\mathbf{w},b,\mathbf{\xi}}}{min} && \frac{1}{2} \mathbf{w}^{T} \mathbf{w} + C \sum_{i=1}^N \xi_i\\
\text{subject to} && y_i (\mathbf{w}^T \phi(\mathbf{x}_i) + b) \geq 1 - \xi_i\\
&& \xi_i \geq 0
\end{aligned}
\end{equation}

* Visualization
The SVM implementation in scikit-learn is used. The first dataset is
[[http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html#sklearn.datasets.make_blobs][make blobs]] which is random points sampled from a gaussian distribution
to experiment with how SVM work and learn the mathematics behind
it. The second set is the [[http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html][iris]] dataset which has noisy points, thereby
allowing the use of SVM in practice.

#+BEGIN_SRC ipython :exports both :async t :results output :session
import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets, svm
import random
import sys
%matplotlib inline
#+END_SRC

Secondly we need to plot the results. The first step is to make a
matrix of points which constituted our mesh with a grid point size of
$h\times h$.

#+BEGIN_SRC ipython :exports both :async t :results output :session
def make_meshgrid(x, y, h=.02):
    """Create a mesh of points to plot in

    Parameters
    ----------
    x: data to base x-axis meshgrid on
    y: data to base y-axis meshgrid on
    h: stepsize for meshgrid, optional

    Returns
    -------
    xx, yy : ndarray
    """
    
    x_min, x_max = x.min() - 1, x.max() + 1
    y_min, y_max = y.min() - 1, y.max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    return xx, yy
#+END_SRC

The second step is to fill the mesh with the contours values. Every
grid point in the mesh is passed to the trained SVM classifier to
determine its label. The result is a contour plot showing the decision
bondaries.

#+BEGIN_SRC ipython :exports both :async t :results output :session
def plot_contours(ax, clf, xx, yy, **params):
    """Plot the decision boundaries for a classifier.

    Parameters
    ----------
    ax: matplotlib axes object
    clf: a classifier
    xx: meshgrid ndarray
    yy: meshgrid ndarray
    params: dictionary of params to pass to contourf, optional
    """

    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    out = ax.contourf(xx, yy, Z, **params)
    return out
#+END_SRC

#+BEGIN_SRC ipython :exports both :async t :results output :session
# we create 40 separable points
blobs = datasets.make_blobs(n_samples=40, centers=2, random_state=6)
X, y = blobs

# fit the model, don't regularize for illustration purposes
clf = svm.SVC(kernel='linear', C=1000)
clf.fit(X, y)

plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)

# plot the decision function
ax = plt.gca()
xlim = ax.get_xlim()
ylim = ax.get_ylim()

# create grid to evaluate model
YY, XX = np.meshgrid(ylim, xlim)
xy = np.vstack([XX.ravel(), YY.ravel()]).T
Z = clf.decision_function(xy).reshape(XX.shape)

# plot decision boundary and margins
ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,
           linestyles=['--', '-', '--'])
# plot support vectors
ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,
           linewidth=1, facecolors='none', edgecolors='k')
plt.show()
#+END_SRC

#+BEGIN_SRC ipython :exports both :async t :results output :session
W = clf.coef_[0]
b = clf.intercept_[0]

# plot the decision function
ax = plt.gca()
ax.set_xlim(-10, 15)
ax.set_ylim(-15, 15)

xlim = ax.get_xlim()
ylim = ax.get_ylim()

# create grid to evaluate model
YY, XX = np.meshgrid(ylim, xlim)
xy = np.vstack([XX.ravel(), YY.ravel()]).T
Z = clf.decision_function(xy).reshape(XX.shape)

# plot decision boundary and margins
ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,
           linestyles=['--', '-', '--'])
# plot support vectors
ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,
           linewidth=1, facecolors='none', edgecolors='k')

weight_x = np.array([0, W[0]])
weight_y = np.array([0, W[1]])

ax.plot(weight_x, weight_y, linewidth=2)
plt.show()
#+END_SRC

* Exploration
All the work done to this point in the blog post is for binary
classification and traditionally SVM was designed for such
problems. Methods exist which allow multi-class classification and a
detailed description can be found [[https://www.csie.ntu.edu.tw/~cjlin/papers/multisvm.pdf][here]]. In general two approaches
exist:
- One vs. the rest: For $k$ classes train $k$ binary classifiers. Each
  classifier tests whether an example belongs to its own class versus
  any other class. The classifier with the largest output is taken to
  be the class of the example.
- One vs. one: For $k$ classes train $k(k âˆ’ 1)/2$ binary
  classifiers. A voting procedure is used to combine the
  outputs. There are many proposals for a voting procedure discussed
  in the above paper.

#+BEGIN_SRC ipython :exports both :async t :results output :session
# import some data to play with
iris = datasets.load_iris()
# Take the first two features. We could avoid this by using a two-dim dataset
X = iris.data[:, :2]
y = iris.target

# we create an instance of SVM and fit out data. We do not scale our
# data since we want to plot the support vectors
C = 1.0  # SVM regularization parameter
models = (svm.SVC(kernel='linear', C=C),
          svm.LinearSVC(C=C),
          svm.SVC(kernel='rbf', gamma=0.7, C=C),
          svm.SVC(kernel='poly', degree=3, C=C))
models = (clf.fit(X, y) for clf in models)

# title for the plots
titles = ('SVC with linear kernel - one vs. one',
          'LinearSVC (linear kernel) - one vs. all',
          'SVC with RBF kernel',
          'SVC with polynomial (degree 3) kernel')
#+END_SRC

#+BEGIN_SRC ipython :exports both :async t :results output :session
# Set-up 2x2 grid for plotting.
fig, sub = plt.subplots(2, 2)
plt.subplots_adjust(wspace=0.4, hspace=0.4)

X0, X1 = X[:, 0], X[:, 1]
xx, yy = make_meshgrid(X0, X1)

for clf, title, ax in zip(models, titles, sub.flatten()):
    plot_contours(ax, clf, xx, yy,
                  cmap=plt.cm.coolwarm, alpha=0.8)
    ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')
    ax.set_xlim(xx.min(), xx.max())
    ax.set_ylim(yy.min(), yy.max())
    ax.set_xlabel('Sepal length')
    ax.set_ylabel('Sepal width')
    ax.set_title(title)

plt.show()
#+END_SRC

The linear models LinearSVC() and SVC(kernel='linear') yield slightly
different decision boundaries. This can be a consequence of the
following differences:

LinearSVC minimizes the squared hinge loss while SVC minimizes the
regular hinge loss.  LinearSVC uses the One-vs-All (also known as
One-vs-Rest) multiclass reduction while SVC uses the One-vs-One
multiclass reduction.

http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html


